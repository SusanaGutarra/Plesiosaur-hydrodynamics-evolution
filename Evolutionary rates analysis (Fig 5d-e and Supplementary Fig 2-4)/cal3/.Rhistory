data.lim=c(0,55),cex.age=1.0, cex.ts=0.9, label="Raw taxic richness")
myclade.tree
# #Generate a set of trees, which is then to be randomly resolved and timescaled
tmp_Hedman_trees <- rep.multiPhylo(myclade.tree, 100)
tmp_Hedman_trees
# #Cal3 timescaling
# #Get rates necessary for cal3 timescaling
# #Using seqTimeList (as in Lloyd et al., 2016) is not necessary, as intervals do not overlap
# #First, we can try the freqRat function (Foote and Raup, 1996), which is just a simple
# #ratio of the frequencies of taxa found in just one, two and three intervals.
# #Let's apply that to "mytimeList.myclade" and tell it to plot the histogram of taxon durations.
freqRat(mytimeList.myclade, plot=TRUE) # #Note, that the frequency distribution of input range data appears to violate
# #model assumptions, producing an impossible freqRat greater than 1 (freqRat: 1.19987)
# #This might explain some of the issues we have in estimating rates
# #Find mean substage length
intLength <- -apply(mytimeList.myclade[[1]],1,diff)
intLength
hist(intLength) # #Note the uneven substage length
meanInt <- mean(intLength)
meanInt # #Our mean substage length
# #Construct likelihood models of the observed frequency of taxon durations
# #These model are necessary to find the instantaneous per-capita extinction 'q' and the
# #per-interval taxonomic sampling probability 'R'
likFun.myclade <- make_durationFreqDisc(mytimeList.myclade, groups = NULL)
if(cal3_rates_approach == "direct"){
# #Find instantaneous per-capita extinction 'q' and per-interval taxonomic sampling probability 'R'
likFun_optim.myclade <- optimPaleo(likFun.myclade)
likFun_optim.myclade # #Optimizer seems to have converged ($convergence = 0), but note that the taxonomic sampling
# #probability R.1 appears to to be very high (0.8005801)! For smaller clades, e.g. Parareptilia, it can be even
# #higher, up to R.1 = 1!
# #We replicate the result for each tree (this is not per se necessary, but by doing this we do not have to rewrite
# #if statements to check whether we use this approach or the alternative one)
likFun_optim.myclade <- rep(list(likFun_optim.myclade), length(tmp_Hedman_trees))
# #We could attempt to change the optimization process
# #We modify our optimization and set maxit higher
# likFun_optim.myclade <- optim(parInit(likFun.myclade), likFun.myclade,
#                               lower = parLower(likFun.myclade), upper = parUpper(likFun.myclade),
#                               method = "L-BFGS-B", control = list(maxit=1000000000))
# likFun_optim.myclade # #But we get nearly the same result for R.1 (0.8005809)
# #We can transform sampling probability to sampling rate using the function sProb2sRate
# #and the mean interval length:
# #We replicate the result again for each tree (this is not per se necessary, but by doing this we do not have
# #to rewrite if statements to check whether we use this approach or the alternative one)
sRate.myclade <- vector()
for (i in 1:length(tmp_Hedman_trees)){
sRate.myclade[i] <- sProb2sRate(likFun_optim.myclade[[i]]$par[2], int.length = meanInt)
}
sRate.myclade # #Instantaneous (per-capita) rate of sampling = 0.4378095. This value still appears to be very
# #high --> indeed, for smaller clades (e.g., Parareptilia) I even got an instantaneous (per-capita) sampling
# #rate of Inf (when R.1 = 1) --> see discussions with Dave Bapst in regards to the frequency distribution of taxon
# #durations (see also Maxwell et al., 2018; Bapst & Hopkins, 2017, p. 53; Sould & Friedman, 2017, p. 172)
# #Is there a way to solve this conundrum? I could use 0.01 for the per-capita sampling rate,
# #as mentioned by Soul & Friedman (2017), but there has to be a better way (?). Am I missing something?
# #In Lloyd et al. (2016) values, which are Inf, are dropped as erroneous, but I don't have a stochastic sequenced
# #time-list to allow for multiple iterations!
} else if(cal3_rates_approach == "literature"){
# #Alternative: constrain the sampling rate based on values derived from the literature
# #Soul and Friedman (2017, p. 181) suggest a per-capita sampling rate on the order of 0.01 Lmy
# #for Paleozoic and Mesozoic terrestrial tetrapods (see also Friedman & Brazeau, 2011; Bapst & Hopkins, 2017, Table 2)
# #According to Bapst & Hopkins (2017, Table 2) the per-capita sampling rates range from
# #0.042 to 0.18 for Devonian tetrapods at genus level (see also Friedman & Brazeau, 2011). As many tetrapods are represented
# #by monospecific genera, these values should (probably?) also apply to the species level. For Dinosauria the sampling rate
# #is 0.018 (D. Bapst in Benson et al., 2018, p. 16; see Lloyd et al., 2016)
# #We generate a uniform distribution with min = 0.018 and max = 0.18 from which we sample our
# #per-capita sampling rates
sRate.myclade <- runif(n = length(tmp_Hedman_trees), min = 0.018, max = 0.18)
# #What is the per-interval taxonomic sampling probability R for a given r.1?
interval.sampling <- sRate2sProb(sRate.myclade, int.length = meanInt) # #per-interval taxonomic sampling probability R
likFun_constraint <- list()
for (i in 1:length(tmp_Hedman_trees)){
tmp_interval.sampling <- interval.sampling[i] # #We have to use a tmp variable, since I cannot specify
# #interval.sampling[i] directly in constrainParPaleo() (will throw an error - have to figure out, why that is the case)
likFun_constraint[[i]] <- constrainParPaleo(likFun.myclade, R.1 ~ tmp_interval.sampling)
}
# #Find instantaneous per-capita extinction 'q'
likFun_optim.myclade <- lapply(likFun_constraint, optimPaleo)
# #We could again attempt to change the optimization process
# #We modify our optimization and set maxit higher
# likFun_constraint_optim <- list()
# for (i in 1:length(tmp_Hedman_trees)){
#   likFun_constraint_optim[[i]] <- optim(parInit(likFun_constraint[[i]]), likFun_constraint[[i]],
#                                         lower = parLower(likFun_constraint[[i]]), upper = parUpper(likFun_constraint[[i]]),
#                                         method = "L-BFGS-B", control = list(maxit=1000000))
#   }
# #Alternatively we could also just fix our sampling rate to 0.01 Lmy (see Sould and Friedman, 2017, p. 181)
# #Constraining sampling rate to 0.01 Lmy
# #r.1=0.01
# #R?
# interval.sampling <- sRate2sProb(0.01, int.length = meanInt)
# likFun_constraint<-constrainParPaleo(likFun.myclade,R.1~interval.sampling)
# likFun_constraint_optim <- optim(parInit(likFun_constraint),likFun_constraint,
#                                  lower=parLower(likFun_constraint),upper=parUpper(likFun_constraint),
#                                  method="L-BFGS-B",control=list(maxit=1000000))
# likFun_constraint_optim # #Hmm, q.1 is now suddenly 0.001 - but this is equal to the lower constraint: (parLower(likFun_constraint))
# #What's going on here?
# #Could this be due to the uneven stage length? See help file for durationFreq:
# #"It is unclear how to treat uneven intervals and I urge workers to consider multiple strategies."
# #Therefore we avoid doing this for now (at least when it comes to a sampling rate of 0.01 Lmy)
}
# #To get the extinction rate (= origination rate; at least, that's what we assume)
# #we simply divide the calculated extinction rate by the interval length.
divRate.myclade <- list()
for(i in 1:length(tmp_Hedman_trees)){
divRate.myclade[[i]] <- likFun_optim.myclade[[i]]$par[1]/meanInt
}
divRate.myclade # # Extinction rate is now per Lmy
# #We could also exclude all infinite values see also Lloyd et al. (2016)
# #But there should not be any (since the sampling rates are fixed between 0.018 and 0.18)
# #or alternatively the estimated one, which is also clearly not Inf
# #So there is currently no need for this
# #Otherwise we would have to exclude those rates - in that case we would have to produce more
# #rate estimates and then sample out of them to match the number of trees (otherwise there would
# #not be enough rate estimates for the cal3 scaling as implemented below)
if(cal3_bin == "NO"){
# #Cal3 timescaling: cal3TimePaleoPhy using minMax and the root.age of nodeDates.resolved
# #Note, that the default cal3 approach adds the terminal ranges, unless you specify FAD.only = TRUE, which is not
# #possible, however, when dateTreatment is set to "minMax" or "randObs"
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeData = myclade.ages,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "minMax",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE)
}
} else if(cal3_bin == "YES"){
if(cal3_dateTreatment == "randObs"){
# #Using bin_cal3TimePaleoPhy (with randObs) instead of cal3TimePaleoPhy (with minMax)
# #Some of the trees used by Cantalapiedra et al. (2017), have been timescaled using this approach
# #Note, however, that this approach assumes that the FAD is treated as a fixed/certain number (only
# #the LAD is treated as being uncertain)
# #Unlike the other cal3 methods, however, the terminal range does not appear to be added, allowing the LAD
# #(or rather the 'time of observation') to vary
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- bin_cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeList = mytimeList.myclade,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "randObs",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE) #Not using randres (for now) + minMax (Lloyd et al., 2016) also did not do that: Why? Ask him!
}
# #An alternative approach would be to use '4 dates' for FAD and LAD, i. e. the lower and upper bound of the FAD
# #and the lower and upper bound of the LAD - since we don't necessarily have the data for this in the ETD, we could
# #just use our current lower and upper bounds of the strat. range of a taxon to provide uniform distributions for
# #the FAD and LAD of our taxa, i. e. FAD: uniform distribution bound by oldest and youngest occurrence of taxon
# #and LAD: uniform distribution bound by oldest and youngest occurrence of taxon --> we sample from both uniform distributions
# #and use the sampled oldest date as FAD and the sampled youngest date as LAD - this way the FAD would no longer be fixed
# #Note, that we could sample by ourself from the uniform distributions or let bin_cal3TimePaleoPhy() do the job - I think the latter
# #does it already to a certain extent using "Start" and "End" dates of our different intervals
} else if(cal3_dateTreatment == "firstLast"){
# #Using bin_cal3TimePaleoPhy (with firstLast) instead of cal3TimePaleoPhy (with minMax)
# #For speciation dynamics analyses
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- bin_cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeList = mytimeList.myclade,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "firstLast",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
FAD.only = cal3_FADonly,
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE) #Not using randres (for now) + minMax (Lloyd et al., 2016) also did not do that: Why? Ask him!
}
}
}
# #Note that we could also timescale every randomly resolved tree several times (like Lloyd et al., 2016), but this seems unnecessary
# #when we just start with one tree, that is randomly resolved (= all random resolutions should be equally represented)
# #It is different, however, if we were to use MPTs or Bayesian posteriors, as the trees there can differ quite much (therefore
# #we can't just timescale one of them a single time) - this might also apply to trees with many polytomies
# #For now we will stick with the easier (and faster) way of timescaling a randomly resolved tree just once (Halliday & Goswami (2016) did
# #that as well)! Note, also that timescaling every randomly resolved tree several times is computationally currently
# #not feasible (at least for the downstream analyses), unless we randomly sample another 100 trees from this set of
# #timescaled trees. But then again, why would you even bother with timescaling the 100 trees multiple times, if you
# #end up with 100 trees anyway - let's just keep it simple for now (consider also unexplored side effects, of random
# #sampling, e.g., would the initial 100 different topologies still all be represented in your new tree sample?)
if(add_cal3_time == "YES"){
# #Unfortunately, the cal3 algorithm tends to produce many branches with zero-lengths
# #BayesTraits should be able to deal with this (since it can deal with polytomies)
# #Other methods, however, cannot deal with zero-length branches
# #A simple workaround would be to add a very small amount of time to the zero-length branches
# #If the added time is small enough, it should not lead to significant change of the tree structure
# #and it avoids the problems associated with zero-length branches
# #We add 0.0001 Myr (= 100 yr) to branches with a zero-length branch
# #If we were to add only 10 years and were to adjust the step size accordingly to 0.0001 instead of 0.001,
# #we would run into RAM memory issues (at least on my local machine and probably on BC3 - BC4 might be able to
# #cope with it)
for(i in 1:length(hedman_trees)){
hedman_trees[[i]]$edge.length[hedman_trees[[i]]$edge.length == 0] <- 0.0001
}
}
# #If we use the cal3 trees, we have to delete the "sampledLogLike" column from our trees object - otherwise
# #a single tree will use up more than 600 Mb (!) when using a step size of 0.0001, which is way to large and
# #causes issues when trying to save our workspace
for(i in 1:length(hedman_trees)){
hedman_trees[[i]]$sampledLogLike <- NULL
}
hedman_trees
# #Set class of hedman_trees to "multiphylo"
class(hedman_trees) <- "multiphylo"
# Stop the clock!:
end <- Sys.time()
# How long did it take?:
end - start_all
roots <- list()
for(i in 1:length(hedman_trees)){
roots[i]<- print(hedman_trees[[i]]$root.time)
}
dateConstraint
sample_these_trees <- which(roots<dateConstraint, arr.ind=TRUE)
sample_these_trees <- sample(sample_these_trees,100, replace = FALSE)
roots
sample_these_trees
roots <- list()
for(i in 1:length(hedman_trees)){
roots[i]<- print(hedman_trees[[i]]$root.time)
}
iterations
iterations = 120 #several analyses have to be done more than once,
# #Generate a set of trees, which is then to be randomly resolved and timescaled
tmp_Hedman_trees <- rep.multiPhylo(myclade.tree, iterations)
tmp_Hedman_trees
# #Cal3 timescaling
# #Get rates necessary for cal3 timescaling
# #Using seqTimeList (as in Lloyd et al., 2016) is not necessary, as intervals do not overlap
# #First, we can try the freqRat function (Foote and Raup, 1996), which is just a simple
# #ratio of the frequencies of taxa found in just one, two and three intervals.
# #Let's apply that to "mytimeList.myclade" and tell it to plot the histogram of taxon durations.
freqRat(mytimeList.myclade, plot=TRUE) # #Note, that the frequency distribution of input range data appears to violate
# #model assumptions, producing an impossible freqRat greater than 1 (freqRat: 1.19987)
# #This might explain some of the issues we have in estimating rates
# #Find mean substage length
intLength <- -apply(mytimeList.myclade[[1]],1,diff)
intLength
hist(intLength) # #Note the uneven substage length
meanInt <- mean(intLength)
meanInt # #Our mean substage length
# #Construct likelihood models of the observed frequency of taxon durations
# #These model are necessary to find the instantaneous per-capita extinction 'q' and the
# #per-interval taxonomic sampling probability 'R'
likFun.myclade <- make_durationFreqDisc(mytimeList.myclade, groups = NULL)
if(cal3_rates_approach == "direct"){
# #Find instantaneous per-capita extinction 'q' and per-interval taxonomic sampling probability 'R'
likFun_optim.myclade <- optimPaleo(likFun.myclade)
likFun_optim.myclade # #Optimizer seems to have converged ($convergence = 0), but note that the taxonomic sampling
# #probability R.1 appears to to be very high (0.8005801)! For smaller clades, e.g. Parareptilia, it can be even
# #higher, up to R.1 = 1!
# #We replicate the result for each tree (this is not per se necessary, but by doing this we do not have to rewrite
# #if statements to check whether we use this approach or the alternative one)
likFun_optim.myclade <- rep(list(likFun_optim.myclade), length(tmp_Hedman_trees))
# #We could attempt to change the optimization process
# #We modify our optimization and set maxit higher
# likFun_optim.myclade <- optim(parInit(likFun.myclade), likFun.myclade,
#                               lower = parLower(likFun.myclade), upper = parUpper(likFun.myclade),
#                               method = "L-BFGS-B", control = list(maxit=1000000000))
# likFun_optim.myclade # #But we get nearly the same result for R.1 (0.8005809)
# #We can transform sampling probability to sampling rate using the function sProb2sRate
# #and the mean interval length:
# #We replicate the result again for each tree (this is not per se necessary, but by doing this we do not have
# #to rewrite if statements to check whether we use this approach or the alternative one)
sRate.myclade <- vector()
for (i in 1:length(tmp_Hedman_trees)){
sRate.myclade[i] <- sProb2sRate(likFun_optim.myclade[[i]]$par[2], int.length = meanInt)
}
sRate.myclade # #Instantaneous (per-capita) rate of sampling = 0.4378095. This value still appears to be very
# #high --> indeed, for smaller clades (e.g., Parareptilia) I even got an instantaneous (per-capita) sampling
# #rate of Inf (when R.1 = 1) --> see discussions with Dave Bapst in regards to the frequency distribution of taxon
# #durations (see also Maxwell et al., 2018; Bapst & Hopkins, 2017, p. 53; Sould & Friedman, 2017, p. 172)
# #Is there a way to solve this conundrum? I could use 0.01 for the per-capita sampling rate,
# #as mentioned by Soul & Friedman (2017), but there has to be a better way (?). Am I missing something?
# #In Lloyd et al. (2016) values, which are Inf, are dropped as erroneous, but I don't have a stochastic sequenced
# #time-list to allow for multiple iterations!
} else if(cal3_rates_approach == "literature"){
# #Alternative: constrain the sampling rate based on values derived from the literature
# #Soul and Friedman (2017, p. 181) suggest a per-capita sampling rate on the order of 0.01 Lmy
# #for Paleozoic and Mesozoic terrestrial tetrapods (see also Friedman & Brazeau, 2011; Bapst & Hopkins, 2017, Table 2)
# #According to Bapst & Hopkins (2017, Table 2) the per-capita sampling rates range from
# #0.042 to 0.18 for Devonian tetrapods at genus level (see also Friedman & Brazeau, 2011). As many tetrapods are represented
# #by monospecific genera, these values should (probably?) also apply to the species level. For Dinosauria the sampling rate
# #is 0.018 (D. Bapst in Benson et al., 2018, p. 16; see Lloyd et al., 2016)
# #We generate a uniform distribution with min = 0.018 and max = 0.18 from which we sample our
# #per-capita sampling rates
sRate.myclade <- runif(n = length(tmp_Hedman_trees), min = 0.018, max = 0.18)
# #What is the per-interval taxonomic sampling probability R for a given r.1?
interval.sampling <- sRate2sProb(sRate.myclade, int.length = meanInt) # #per-interval taxonomic sampling probability R
likFun_constraint <- list()
for (i in 1:length(tmp_Hedman_trees)){
tmp_interval.sampling <- interval.sampling[i] # #We have to use a tmp variable, since I cannot specify
# #interval.sampling[i] directly in constrainParPaleo() (will throw an error - have to figure out, why that is the case)
likFun_constraint[[i]] <- constrainParPaleo(likFun.myclade, R.1 ~ tmp_interval.sampling)
}
# #Find instantaneous per-capita extinction 'q'
likFun_optim.myclade <- lapply(likFun_constraint, optimPaleo)
# #We could again attempt to change the optimization process
# #We modify our optimization and set maxit higher
# likFun_constraint_optim <- list()
# for (i in 1:length(tmp_Hedman_trees)){
#   likFun_constraint_optim[[i]] <- optim(parInit(likFun_constraint[[i]]), likFun_constraint[[i]],
#                                         lower = parLower(likFun_constraint[[i]]), upper = parUpper(likFun_constraint[[i]]),
#                                         method = "L-BFGS-B", control = list(maxit=1000000))
#   }
# #Alternatively we could also just fix our sampling rate to 0.01 Lmy (see Sould and Friedman, 2017, p. 181)
# #Constraining sampling rate to 0.01 Lmy
# #r.1=0.01
# #R?
# interval.sampling <- sRate2sProb(0.01, int.length = meanInt)
# likFun_constraint<-constrainParPaleo(likFun.myclade,R.1~interval.sampling)
# likFun_constraint_optim <- optim(parInit(likFun_constraint),likFun_constraint,
#                                  lower=parLower(likFun_constraint),upper=parUpper(likFun_constraint),
#                                  method="L-BFGS-B",control=list(maxit=1000000))
# likFun_constraint_optim # #Hmm, q.1 is now suddenly 0.001 - but this is equal to the lower constraint: (parLower(likFun_constraint))
# #What's going on here?
# #Could this be due to the uneven stage length? See help file for durationFreq:
# #"It is unclear how to treat uneven intervals and I urge workers to consider multiple strategies."
# #Therefore we avoid doing this for now (at least when it comes to a sampling rate of 0.01 Lmy)
}
# #To get the extinction rate (= origination rate; at least, that's what we assume)
# #we simply divide the calculated extinction rate by the interval length.
divRate.myclade <- list()
for(i in 1:length(tmp_Hedman_trees)){
divRate.myclade[[i]] <- likFun_optim.myclade[[i]]$par[1]/meanInt
}
divRate.myclade # # Extinction rate is now per Lmy
# #We could also exclude all infinite values see also Lloyd et al. (2016)
# #But there should not be any (since the sampling rates are fixed between 0.018 and 0.18)
# #or alternatively the estimated one, which is also clearly not Inf
# #So there is currently no need for this
# #Otherwise we would have to exclude those rates - in that case we would have to produce more
# #rate estimates and then sample out of them to match the number of trees (otherwise there would
# #not be enough rate estimates for the cal3 scaling as implemented below)
if(cal3_bin == "NO"){
# #Cal3 timescaling: cal3TimePaleoPhy using minMax and the root.age of nodeDates.resolved
# #Note, that the default cal3 approach adds the terminal ranges, unless you specify FAD.only = TRUE, which is not
# #possible, however, when dateTreatment is set to "minMax" or "randObs"
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeData = myclade.ages,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "minMax",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE)
}
} else if(cal3_bin == "YES"){
if(cal3_dateTreatment == "randObs"){
# #Using bin_cal3TimePaleoPhy (with randObs) instead of cal3TimePaleoPhy (with minMax)
# #Some of the trees used by Cantalapiedra et al. (2017), have been timescaled using this approach
# #Note, however, that this approach assumes that the FAD is treated as a fixed/certain number (only
# #the LAD is treated as being uncertain)
# #Unlike the other cal3 methods, however, the terminal range does not appear to be added, allowing the LAD
# #(or rather the 'time of observation') to vary
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- bin_cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeList = mytimeList.myclade,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "randObs",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE) #Not using randres (for now) + minMax (Lloyd et al., 2016) also did not do that: Why? Ask him!
}
# #An alternative approach would be to use '4 dates' for FAD and LAD, i. e. the lower and upper bound of the FAD
# #and the lower and upper bound of the LAD - since we don't necessarily have the data for this in the ETD, we could
# #just use our current lower and upper bounds of the strat. range of a taxon to provide uniform distributions for
# #the FAD and LAD of our taxa, i. e. FAD: uniform distribution bound by oldest and youngest occurrence of taxon
# #and LAD: uniform distribution bound by oldest and youngest occurrence of taxon --> we sample from both uniform distributions
# #and use the sampled oldest date as FAD and the sampled youngest date as LAD - this way the FAD would no longer be fixed
# #Note, that we could sample by ourself from the uniform distributions or let bin_cal3TimePaleoPhy() do the job - I think the latter
# #does it already to a certain extent using "Start" and "End" dates of our different intervals
} else if(cal3_dateTreatment == "firstLast"){
# #Using bin_cal3TimePaleoPhy (with firstLast) instead of cal3TimePaleoPhy (with minMax)
# #For speciation dynamics analyses
hedman_trees <- foreach(i = 1:length(tmp_Hedman_trees), .packages = "paleotree", .errorhandling = "remove") %dopar% {
out <- bin_cal3TimePaleoPhy(tree = tmp_Hedman_trees[[i]], timeList = mytimeList.myclade,
brRate = divRate.myclade[[i]], extRate = divRate.myclade[[i]], sampRate = sRate.myclade[i],
#node.mins = nodeDates.resolved, # #Cal3 does not require to specify a min. root age
#randres=TRUE, #Randres could be used if we were to resolve polytomies within cal3 and if we did not
# #want to account for direct ancestors (sparse record of tetrapods is unlikely to contain those) # #We use randres, as it is unlikely that the sparse record of tetrapods contains ancestors + it retains our node labels
dateTreatment = "firstLast",
step.size = 0.001, # #Reducing the step size of increments used in zipper algorithm to assign node ages
# #leads to fewer branches with zero-length
FAD.only = cal3_FADonly,
#add.term = FALSE, # #add.term does not work with cal3 method
anc.wt = 0, # #It's very unlikely that ancestors have been sampled in the fossil record of early tetrapods!
# #Therefore no taxon is allowed to be an ancestor.
ntrees = 1, plot = FALSE) #Not using randres (for now) + minMax (Lloyd et al., 2016) also did not do that: Why? Ask him!
}
}
}
# #Note that we could also timescale every randomly resolved tree several times (like Lloyd et al., 2016), but this seems unnecessary
# #when we just start with one tree, that is randomly resolved (= all random resolutions should be equally represented)
# #It is different, however, if we were to use MPTs or Bayesian posteriors, as the trees there can differ quite much (therefore
# #we can't just timescale one of them a single time) - this might also apply to trees with many polytomies
# #For now we will stick with the easier (and faster) way of timescaling a randomly resolved tree just once (Halliday & Goswami (2016) did
# #that as well)! Note, also that timescaling every randomly resolved tree several times is computationally currently
# #not feasible (at least for the downstream analyses), unless we randomly sample another 100 trees from this set of
# #timescaled trees. But then again, why would you even bother with timescaling the 100 trees multiple times, if you
# #end up with 100 trees anyway - let's just keep it simple for now (consider also unexplored side effects, of random
# #sampling, e.g., would the initial 100 different topologies still all be represented in your new tree sample?)
if(add_cal3_time == "YES"){
# #Unfortunately, the cal3 algorithm tends to produce many branches with zero-lengths
# #BayesTraits should be able to deal with this (since it can deal with polytomies)
# #Other methods, however, cannot deal with zero-length branches
# #A simple workaround would be to add a very small amount of time to the zero-length branches
# #If the added time is small enough, it should not lead to significant change of the tree structure
# #and it avoids the problems associated with zero-length branches
# #We add 0.0001 Myr (= 100 yr) to branches with a zero-length branch
# #If we were to add only 10 years and were to adjust the step size accordingly to 0.0001 instead of 0.001,
# #we would run into RAM memory issues (at least on my local machine and probably on BC3 - BC4 might be able to
# #cope with it)
for(i in 1:length(hedman_trees)){
hedman_trees[[i]]$edge.length[hedman_trees[[i]]$edge.length == 0] <- 0.0001
}
}
# #If we use the cal3 trees, we have to delete the "sampledLogLike" column from our trees object - otherwise
# #a single tree will use up more than 600 Mb (!) when using a step size of 0.0001, which is way to large and
# #causes issues when trying to save our workspace
for(i in 1:length(hedman_trees)){
hedman_trees[[i]]$sampledLogLike <- NULL
}
hedman_trees
# #Set class of hedman_trees to "multiphylo"
class(hedman_trees) <- "multiphylo"
# Stop the clock!:
end <- Sys.time()
# How long did it take?:
end - start_all
roots <- list()
for(i in 1:length(hedman_trees)){
roots[i]<- print(hedman_trees[[i]]$root.time)
}
sample_these_trees <- which(roots<dateConstraint, arr.ind=TRUE)
dateConstraint
length(sample_these_trees)
sample_these_trees <- sample(sample_these_trees,100, replace = FALSE)
sample_these_trees
myclade.ts.trees <- hedman_trees[sample_these_trees]
myclade.ts.trees
for(i in 1:length(myclade.ts.trees)){
print(myclade.ts.trees[[i]]$root.time)
}
check.branches <- data.frame(matrix(ncol = length(myclade.ts.trees), nrow = length(myclade.ts.trees[[1]]$edge.length)))
for(i in 1:length(myclade.ts.trees)){
check.branches[,i] <- myclade.ts.trees[[i]]$edge.length
}
check.branches
unlist(check.branches)
min(unlist(check.branches))
max(unlist(check.branches))
length(which(unlist(check.branches)==0.0001))/length(unlist(check.branches))
hist(unlist(check.branches), breaks = 1000)
abline(v=1, col="red")
less_than_1myr <- sum(unlist(check.branches) < 1)
total <- length(unlist(check.branches))
prop_than_1myr <- less_than_1myr/total*100
zero_length <- sum(unlist(check.branches) == 0)
text(50, 1000, labels = prop_than_1myr)
text(50, 700, labels = zero_length)
pick <- sample(1:100, 1)
pick
pick2 <- sample(1:100, 1)
pick2
pick3 <- sample(1:100, 1)
pick3
pick4 <- sample(1:100, 1)
pick4
geoscalePhylo(ladderize(myclade.ts.trees[[pick]]), cex.tip = 0.3)
dev.copy(pdf,'sample_dated_supertree_1.pdf', width = 15, height = 15)
dev.off()
geoscalePhylo(ladderize(myclade.ts.trees[[pick2]]), cex.tip = 0.3)
dev.copy(pdf,'sample_dated_supertree_2.pdf', width = 15, height = 15)
dev.off()
geoscalePhylo(ladderize(myclade.ts.trees[[pick3]]), cex.tip = 0.3)
dev.copy(pdf,'sample_dated_supertree_3.pdf', width = 15, height = 15)
dev.off()
geoscalePhylo(ladderize(myclade.ts.trees[[pick4]]), cex.tip = 0.3)
dev.copy(pdf,'sample_dated_supertree_4.pdf', width = 15, height = 15)
dev.off()
check.nodes <- data.frame(matrix(ncol = length(myclade.ts.trees), nrow = length(node.depth.edgelength(myclade.ts.trees[[1]]))))
for(i in 1:length(myclade.ts.trees)){
check.nodes[,i] <- node.depth.edgelength(myclade.ts.trees[[i]])
}
check.nodes
min(unlist(check.nodes))
max(unlist(check.nodes))
save(myclade.ts.trees, file = "cal3trees.RData")
saveRDS(myclade.ts.trees, "cal3trees.rds")
write.nexus(myclade.ts.trees, file = "cal3trees.nex", translate = TRUE)
